---
title: "Forecasting & Predictive Group Assignment"
author: "Kai KANG, Jiaqian MA, Yinzhe HUANG, Yue CHEN"
date: "2022/1/12"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r include=FALSE}
# install.packages('tidyverse')
library(tidyverse)
rawData = read_csv("./dat/Projectdata.csv")
# spec(rawData)
```



### 1.2 We now want to assess the set of benchmark in the M5 guidelines. For several sample splits of your choice,

#### (a) Fit Statistical benchmarks 1 (Naive), 2 (sNaive), 3 (ES), 4 (MA), 13 (ESX), as well
#### as SARIMA, SARIMAX (using X as in ESX) and Holt-Winters up to the end of the
#### training subsample.

##### ES
```{r warning=False} 
ts_list = lapply(trainData[,2:ncol(rawData)], ts)
for (h in 1:28) {
  ses_pred_list = lapply(ts_list, ses, h=h)
}
```

##### ESX
```{r warning=False} 
ts_list = lapply(trainData[,2:ncol(rawData)], ts)
for (h in 1:28) {
  esx_pred_list = lapply(ts_list, ses, h=h, xreg = x_var_train$holiday)
}
```

##### Holt-Winters
```{r warning=False} 
ts_list = lapply(trainData[,2:ncol(rawData)], ts)
hw_list = lapply(ts_list, ets)
for (h in 1:28) {
  esx_pred_list = lapply(hw_list, forecast.beta, h = h)
}
```

#### (c) Obtain a sequence of 1-step ahead point forecasts over the testing subsample.
```{r}
es_pred_list = lapply(ts_list, ses, h=1)
print(es_pred_list)
hw_pred_list = lapply(hw_list, forecast.beta, h = 1)
print(hw_pred_list)
```





#### (d) Assess the quality of the forecasts using relevant loss functions.




#### (e) Compare the above with combinations of the forecasting techniques (using simple averages across methods, such as Combination benchmark #21 in the M5 guidelines).
```{r}
source('./arima_RMSSE_v.RData')
```



#### (f) Compare the in-sample (training) vs. out-of-sample (testing) fit of the models.




#### (g) Do you find similarities in terms of forecast performance across stores or types of items?
