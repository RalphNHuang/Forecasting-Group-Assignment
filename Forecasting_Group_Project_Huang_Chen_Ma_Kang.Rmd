---
title: "Forecasting Group Project"
author: "Yue Chen, Kai Kang, Jiaqian Ma, Yinzhe Huang"
date: "2022/1/1"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r include=F}
source("./R/utils.R")
source("./R/dataUtils.R")
load('./.RData')

for (i in dir("./RData/")){
  path = paste("./RData/", i, sep = "")
  load(path)
}

for (i in dir("./1.2.f/")){
  path = paste("./1.2.f/", i, sep = "")
  load(path)
}

```

Scale raw data before calculating.
With function scale, we "scale" each element by those values by subtracting the mean and dividing by the standard deviation. In this way, time series are more stable.

## 1 Assessing the data and univariate benchmarks
### 1.1 Are these six variables stationary,
#### (a) using a simple method, e.g. looking at the time series plots and at the ACF/PACF?
##### ACF & PACF of Hoobies category
```{r}
par(mfrow=c(3,2))
acf(rawData$Hobbies_CA_1)
pacf(rawData$Hobbies_CA_1)
acf(rawData$Hobbies_CA_2)
pacf(rawData$Hobbies_CA_2)
acf(rawData$Hobbies_CA_3)
pacf(rawData$Hobbies_CA_3)
```

##### ACF & PACF of Household_1 category
```{r}
par(mfrow=c(3,2))
acf(rawData$Household_1_CA_1)
pacf(rawData$Household_1_CA_1)
acf(rawData$Household_1_CA_2)
pacf(rawData$Household_1_CA_2)
acf(rawData$Household_1_CA_3)
pacf(rawData$Household_1_CA_3)
```

##### ACF & PACF of Household_2 category
```{r}
par(mfrow=c(3,2))
acf(rawData$Household_2_CA_1)
pacf(rawData$Household_2_CA_1)
acf(rawData$Household_2_CA_2)
pacf(rawData$Household_2_CA_2)
acf(rawData$Household_2_CA_3)
pacf(rawData$Household_2_CA_3)
```

##### ACF & PACF of Foods_1 category
```{r}
par(mfrow=c(3,2))
acf(rawData$Foods_1_CA_1)
pacf(rawData$Foods_1_CA_1)
acf(rawData$Foods_1_CA_2)
pacf(rawData$Foods_1_CA_2)
acf(rawData$Foods_1_CA_3)
pacf(rawData$Foods_1_CA_3)
```

##### ACF & PACF of Food_2 category
```{r}
par(mfrow=c(3,2))
acf(rawData$Foods_2_CA_1)
pacf(rawData$Foods_2_CA_1)
acf(rawData$Foods_2_CA_2)
pacf(rawData$Foods_2_CA_2)
acf(rawData$Foods_2_CA_3)
pacf(rawData$Foods_2_CA_3)
```

##### ACF & PACF of Foods_3 category
```{r}
par(mfrow=c(3,2))
acf(rawData$Foods_3_CA_1)
pacf(rawData$Foods_3_CA_1)
acf(rawData$Foods_3_CA_2)
pacf(rawData$Foods_3_CA_2)
acf(rawData$Foods_3_CA_3)
pacf(rawData$Foods_3_CA_3)
```
In Hobbies_CA_1,Foods_1_CA_1, Foods_2_CA_1, Hobbies_CA_2, Household_2_CA_3, Foods_1_CA_3, Foods_2_CA_3,  the ACF tails off after the first lag, PACF tails off after the 5th lag.
In Household_1_CA_1, Foods_3_CA_1,Household_1_CA_2, Household_2_CA_2, Foods_3_CA_2, Foods_3_CA_3, the ACF tails off after the first lag, PACF tails off after the 5th lag,  with seasonality 365 days.
In Household_2_CA_1 the ACF tails off after the first lag, PACF tails off after the 5th lag,  with seasonality 365 days and drift.
In Foods_2_CA_2 the ACF tails off after the second lag, PACF tails off after the 5th lag with seasonality 365 days.
In Foods_1_CA_2, the ACF is neither tail off nor cut off.
In Hobbies_CA_3,  the ACF tails off after the second lag, PACF tails off after the 5th lag with drift.


#### (b) performing a test?
We used ADF test on each time series.
##### ADF Test for Hobbies category
```{r}
library(tseries)
adf.test(rawData$Hobbies_CA_1)
adf.test(rawData$Hobbies_CA_2)
adf.test(rawData$Hobbies_CA_3)
```

##### ADF Test for Houshold_1 category
```{r}
library(tseries)
adf.test(rawData$Household_1_CA_1)
adf.test(rawData$Household_1_CA_2)
adf.test(rawData$Household_1_CA_3)
```

##### ADF Test for Houshold_2 category
```{r}
library(tseries)
adf.test(rawData$Household_2_CA_1)
adf.test(rawData$Household_2_CA_2)
adf.test(rawData$Household_2_CA_3)
```

##### ADF Test for Foods_1 category
```{r}
library(tseries)
adf.test(rawData$Foods_1_CA_1)
adf.test(rawData$Foods_1_CA_2)
adf.test(rawData$Foods_1_CA_3)
```

##### ADF Test for Foods_2 category
```{r}
library(tseries)
adf.test(rawData$Foods_2_CA_1)
adf.test(rawData$Foods_2_CA_2)
adf.test(rawData$Foods_2_CA_3)
```

##### ADF Test for Foods_3 category
```{r}
library(tseries)
adf.test(rawData$Foods_3_CA_1)
adf.test(rawData$Foods_3_CA_1)
adf.test(rawData$Foods_3_CA_1)
```
We used ADF test on each time series. Based on the result of the ADF test on each column, except for p-value of `Foods_1_CA_2`, p-value of the rest column is less than 0.05, which means that the other 17 time series data are stationary. 



### 1.2 We now want to assess the set of benchmark in the M5 guidelines. For several sample splits of your choice,
#### (a) Fit Statistical benchmarks 1 (Naive), 2 (sNaive), 3 (ES), 4 (MA), 13 (ESX), as well
#### as SARIMA, SARIMAX (using X as in ESX) and Holt-Winters up to the end of the
#### training subsample.
```{r}
# you can choose to summary different model(by change model_list) with different product(by change [1])
print(summary(ma_list[[1]]))
print(summary(arima_list[[2]]))
print(summary(arimax_list_snap[[3]]))
print(summary(hw_list[[4]]))
print(summary(tbats_list[[5]]))
```

#### (b) Consider also a state-space model where the latent state variable follows a random walk.
#### Have a look as well at the TBATS package in R.
```{r}
print(summary(tbats_list[[1]]))
```

#### (c) Obtain a sequence of 1-step ahead point forecasts over the testing subsample.
```{r}
print(hybrid_crossH_RMSSE)
```
#### (d) Assess the quality of the forecasts using relevant loss functions.
We used RMSSE to evaluate the performance of each forecast model.
```{r}
cbind(naive_RMSSE_v
,snaive_RMSSE_v
,ses_RMSSE_v
,ma_RMSSE_v
,esx_holiday_RMSSE_v
,esx_snap_RMSSE_v
,arima_RMSSE_v
,arimax_snap_RMSSE_v
,hw_RMSSE_v
,tbats_RMSSE_v
)
```
In all, the state-space model has the least RMSSE, Holt-Winters, SARIMA, SARIMAX are basically the same, which are better than ESX, MA, ES. Naive and sNaive perform the worst.

#### (e) Compare the above with combinations of the forecasting techniques (using simple averages across methods, such as Combination benchmark #21 in the M5 guidelines)
```{r}
print(hybrid_crossH_RMSSE)
```
We combine the ARIMA and ES models to get the hybrid model. Compared to the previous model, it’s worse than the state-space model, but better than the rest of the models. 

#### (f) Compare the in-sample (training) vs. out-of-sample (testing) fit of the models.
```{r}
name_list = names(trainData[,2:19])

data.frame(cbind(naive_RMSE_train_v,naive_RMSE_test_v),row.names = name_list)
data.frame(cbind(snaive_RMSE_train_v,snaive_RMSE_test_v),row.names = name_list)
data.frame(cbind(ses_RMSE_train_v,ses_RMSE_test_v),row.names = name_list)
data.frame(cbind(ma_RMSE_train_v,ma_RMSE_test_v),row.names = name_list)
data.frame(cbind(esx_holiday_RMSE_train_v,esx_holiday_RMSE_test_v),row.names = name_list)
data.frame(cbind(esx_snap_RMSE_train_v,esx_snap_RMSE_test_v),row.names = name_list)
data.frame(cbind(arima_RMSE_train_v,arima_RMSE_test_v),row.names = name_list)
data.frame(cbind(arimax_RMSE_train_v,arimax_RMSE_test_v),row.names = name_list)
data.frame(cbind(hw_RMSE_train_v,hw_RMSE_test_v),row.names = name_list)
data.frame(cbind(tbats_RMSE_train_v,tbats_RMSE_test_v),row.names = name_list)
```
In-sample (training) is more fit than out-of-sample (testing) of the Naive, ES, Holt-Winters, state space models
sNaive, ESX, SARIMA, SARIMAX models have similar errors on In-sample (training) and out-of-sample (testing).

#### (g) Do you find similarities in terms of forecast performance across stores or types of items?
No

### 1.3 Consider varying the horizon, h, and obtaining forecasts for h = 1, ..., 28. Do you find different
### rankings of models according to the RMSSE (Root Mean Square Scaled Error) suggested in
### the M5 guidelines.
```{r}
print(naive_crossH_RMSSE)
print(snaive_crossH_RMSSE)
print(ses_crossH_RMSSE)
print(esx_holiday_crossH_RMSSE)
print(esx_snap_crossH_RMSSE)
print(arima_crossH_RMSSE)
print(arimax_crossH_RMSSE)
print(hw_crossH_RMSSE)
print(tbats_crossH_RMSSE)

```
Based on RMSSE results, we can know that the order of model performance: state-space > Holt-Winters = SARIMA = SARIMAX > ESX = MA = ES = Naive

### 1.4 Now, in this question only, aggregate the data at the store or type of item level.
#### (a) Produce forecasts for these aggregates using the previous methods and assess them.
```{r}
# model
print(summary(ma_cat_list[[1]]))
print(summary(cat_ses_list[[1]]))
print(summary(cat_esx_holiday_list[[1]]))
print(summary(cat_esx_snap_list[[1]]))
print(summary(cat_hw_list[[1]]))
print(summary(cat_tbats_list[[1]]))

```
state-space > Holt-Winters = SARIMA = SARIMAX > ESX > MA = ES = Naive

#### (b) Compare the forecasts of the aggregates to the aggregates of the forecasts that you
#### had obtained in previous questions, can you find systematic pattern in terms of relative
#### forecasting performance?
```{r}
print(cbind(naive_cat_RMSSE_v
,snaive_cat_RMSSE_v
,ma_cat_RMSSE_v
,cat_ses_RMSSE_v
,cat_esx_holiday_RMSSE_v
,cat_esx_snap_RMSSE_v
,cat_arima_RMSSE_v
,cat_arimax_RMSSE_v
,cat_hw_RMSSE_v
,cat_tbats_RMSSE_v
))

```
First, we calculate the arithmetic mean of RMSSE from previous forecast models, and compare them with the results from aggregated forecast models.Use the state-space model as example, values of forecasts of the aggregates are smaller than the values of aggregates of the previous forecasts.  For Hobbies, aggregates of the previous forecasts and forecasts of the aggregates are 0.6, 0.54, for foods are 0.76, 0.56, for Households are 0.63 and 0.42.
Thus, we can infer aggregate before forecasting will result in better forecasting performance.


### 1.5 Now, in this question only, aggregate the data at the weekly frequency,
#### (a) produce forecasts for the weekly aggregates using the previous methods and assess them
```{r}
# model
print(summary(ma_week_list[[1]]))
print(summary(week_ses_list[[1]]))
print(summary(ma_week_list[[1]]))
print(summary(week_esx_holiday_list[[1]]))
print(summary(week_esx_snap_list[[1]]))
print(summary(week_hw_list[[1]]))
print(summary(week_tbats_list[[1]]))
```
#### (b) Compare the forecasts of the weekly aggregates vs. the weekly aggregates of the daily
#### forecasts, can you find systematic pattern?
```{r}
print(cbind(naive_week_RMSSE_v
,snaive_week_RMSSE_v
,ma_week_RMSSE_v
,week_ses_RMSSE_v
,week_esx_holiday_RMSSE_v
,week_esx_snap_RMSSE_v
,week_hw_RMSSE_v
,week_tbats_RMSSE_v
))
```
Use the Holt Winter model as example, the weekly aggregates of the daily forecast RMSSE is 2.56 and the forecasts of the weekly aggregates RMSSE is 0.9 which is way smaller than the former. So as the MAE and ME.

### 1.6 OPTIONAL: Based on your answers to questions 4 and 5 above, see whether using lags of
#### the aggregates can help forecasting the daily data. When using lags of temporal aggregates,
#### this is related to the HAR model à la Corsi (2009).1 You may want also to consider an
#### ARFIMA(p, d, q) model where d ∈ (0, 1).



### 1.7 We now only focus on forecasting the disaggregates (original data).
#### (a) Consider the probabilistic forecasts i to vi in the M5 guidelines
#### (b) Assess them using the Scaled Pinball Loss (p7 of the M5 guidelines).








