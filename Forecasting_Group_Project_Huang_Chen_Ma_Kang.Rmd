---
title: "Forecasting Group Project"
author: "Yue Chen, Kai Kang, Jiaqian Ma, Yinzhe Huang"
date: "2022/1/1"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r include=F}
source("./R/utils.R")
source("./R/dataUtils.R")
load('./.RData')

for (i in dir("./RData/")){
  path = paste("./RData/", i, sep = "")
  load(path)
}

for (i in dir("./1.2.f/")){
  path = paste("./1.2.f/", i, sep = "")
  load(path)
}

```

## 1 Assessing the data and univariate benchmarks
### 1.1 Are these six variables stationary,
#### (a) using a simple method, e.g. looking at the time series plots and at the ACF/PACF?
```{r: ACF&PACF}

```
#### (b) performing a test?

```{r: ADF test}

```
Based on the result of ADF test on each column, expect for p-value of `Foods_1_CA_2`, p-value of the rest column is less than 0.05, which means that the other 17 time series data is stationary. 

### 1.2 We now want to assess the set of benchmark in the M5 guidelines. For several sample splits of your choice,
#### (a) Fit Statistical benchmarks 1 (Naive), 2 (sNaive), 3 (ES), 4 (MA), 13 (ESX), as well
#### as SARIMA, SARIMAX (using X as in ESX) and Holt-Winters up to the end of the
#### training subsample.
```{r}
# you can choose to summary different model(by change model_list) with different product(by change [1])
print(summary(ma_list[[1]]))
print(summary(arima_list[[2]]))
print(summary(arimax_list_snap[[3]]))
print(summary(hw_list[[4]]))
print(summary(tbats_list[[5]]))
```
#### (b) Consider also a state-space model where the latent state variable follows a random walk.
#### Have a look as well at the TBATS package in R.
```{r}

```
#### (c) Obtain a sequence of 1-step ahead point forecasts over the testing subsample.
```{r}

```
#### (d) Assess the quality of the forecasts using relevant loss functions.
We used RMSSE to evaluate the performance of each forecast model.
```{r}
cbind(naive_RMSSE_v
,snaive_RMSSE_v
,ses_RMSSE_v
,ma_RMSSE_v
,esx_holiday_RMSSE_v
,esx_snap_RMSSE_v
,arima_RMSSE_v
,arimax_snap_RMSSE_v
,hw_RMSSE_v
,tbats_RMSSE_v
)
```
#### (e) Compare the above with combinations of the forecasting techniques (using simple averages across methods, such as Combination benchmark #21 in the M5 guidelines)
```{r}

```
#### (f) Compare the in-sample (training) vs. out-of-sample (testing) fit of the models.
```{r}
name_list = names(trainData[,2:19])

data.frame(cbind(naive_RMSE_train_v,naive_RMSE_test_v),row.names = name_list)
data.frame(cbind(snaive_RMSE_train_v,snaive_RMSE_test_v),row.names = name_list)
data.frame(cbind(ses_RMSE_train_v,ses_RMSE_test_v),row.names = name_list)
data.frame(cbind(ma_RMSE_train_v,ma_RMSE_test_v),row.names = name_list)
data.frame(cbind(esx_holiday_RMSE_train_v,esx_holiday_RMSE_test_v),row.names = name_list)
data.frame(cbind(esx_snap_RMSE_train_v,esx_snap_RMSE_test_v),row.names = name_list)
data.frame(cbind(arima_RMSE_train_v,arima_RMSE_test_v),row.names = name_list)
data.frame(cbind(arimax_RMSE_train_v,arimax_RMSE_test_v),row.names = name_list)
data.frame(cbind(hw_RMSE_train_v,hw_RMSE_test_v),row.names = name_list)
data.frame(cbind(tbats_RMSE_train_v,tbats_RMSE_test_v),row.names = name_list)
```
#### (g) Do you find similarities in terms of forecast performance across stores or types of items?



### 1.3 Consider varying the horizon, h, and obtaining forecasts for h = 1, ..., 28. Do you find different
### rankings of models according to the RMSSE (Root Mean Square Scaled Error) suggested in
### the M5 guidelines.
```{r}
print(naive_crossH_RMSSE)
print(ses_crossH_RMSSE)
print(esx_holiday_crossH_RMSSE)
print(esx_snap_crossH_RMSSE)
print(arima_crossH_RMSSE)
print(arimax_crossH_RMSSE)
print(hw_crossH_RMSSE)
print(tbats_crossH_RMSSE)

```

### 1.4 Now, in this question only, aggregate the data at the store or type of item level.
#### (a) Produce forecasts for these aggregates using the previous methods and assess them.
```{r}
# model
print(summary(ma_cat_list[[1]]))
print(summary(cat_ses_list[[1]]))
print(summary(cat_esx_holiday_list[[1]]))
print(summary(cat_esx_snap_list[[1]]))
print(summary(cat_hw_list[[1]]))
print(summary(cat_tbats_list[[1]]))
```

#### (b) Compare the forecasts of the aggregates to the aggregates of the forecasts that you
#### had obtained in previous questions, can you find systematic pattern in terms of relative
#### forecasting performance?
```{r}
print(cbind(naive_cat_RMSSE_v
,snaive_cat_RMSSE_v
,ma_cat_RMSSE_v
,cat_ses_RMSSE_v
,cat_esx_holiday_RMSSE_v
,cat_esx_snap_RMSSE_v
,cat_arima_RMSSE_v
,cat_arimax_RMSSE_v
,cat_hw_RMSSE_v
,cat_tbats_RMSSE_v
))
```


### 1.5 Now, in this question only, aggregate the data at the weekly frequency,
#### (a) produce forecasts for the weekly aggregates using the previous methods and assess them
```{r}
# model
print(summary(ma_week_list[[1]]))
print(summary(week_ses_list[[1]]))
print(summary(ma_week_list[[1]]))
print(summary(week_esx_holiday_list[[1]]))
print(summary(week_esx_snap_list[[1]]))
print(summary(week_hw_list[[1]]))
print(summary(week_tbats_list[[1]]))
```
#### (b) Compare the forecasts of the weekly aggregates vs. the weekly aggregates of the daily
#### forecasts, can you find systematic pattern?
```{r}
print(cbind(naive_week_RMSSE_v
,snaive_week_RMSSE_v
,ma_week_RMSSE_v
,week_ses_RMSSE_v
,week_esx_holiday_RMSSE_v
,week_esx_snap_RMSSE_v
,week_arima_RMSSE_v
,week_arimax_RMSSE_v
,week_hw_RMSSE_v
,week_tbats_RMSSE_v
))
```


### 1.6 OPTIONAL: Based on your answers to questions 4 and 5 above, see whether using lags of
#### the aggregates can help forecasting the daily data. When using lags of temporal aggregates,
#### this is related to the HAR model à la Corsi (2009).1 You may want also to consider an
#### ARFIMA(p, d, q) model where d ∈ (0, 1).



### 1.7 We now only focus on forecasting the disaggregates (original data).
#### (a) Consider the probabilistic forecasts i to vi in the M5 guidelines
#### (b) Assess them using the Scaled Pinball Loss (p7 of the M5 guidelines).








## 2 Multivariate Models

### 2.1 Consider a VAR model for each store and obtain resulting forecasts for the testing sample



### 2.2 Consider a VAR model for each type of products and obtain forecasts.



### 2.3 Consider a large VAR model involving all variables.



### 2.4 OPTIONAL

#### (a) If you wish, you may consider dynamic factor models where you allow the possibility of
#### one factor for each store or each type of item.

#### (b) If you wish, you may use the whole dataset available on github and consider a large
#### dynamic factor model.









## 3 Machine/Deep Learning models and extension

### 3.1 Now consider machine learning benchmarks, such as those in the set of benchmarks or that
### are available on the M5 github (e.g. LSTM, using keras in R) and compare their performance
### in terms of point/probabilistic forecasts.




### 3.2 Can combinations of ML/DL techniques with standard techniques help?